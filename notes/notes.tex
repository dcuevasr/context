\documentclass{report}
\usepackage{graphicx}
\usepackage{breakcites}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tabularx}
\usepackage[margin=1.5cm]{geometry}
\usepackage{todonotes}
\bibliographystyle{apalike}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

% Defining parameters
\def \parts {60 }                  % Number of participants
\def \fref #1{Figure \ref{#1}}     % Reference figures
\def \tref #1{Table \ref{#1}}      % Reference tables
\def \cents {20 }                  % Cents earned per mini-block
\def \eref #1{Equation \ref{#1}}   % Reference equations
\def \sref #1{Section '\nameref{#1}'}    % Reference sections
\def \supmat {the Supplementary Materials}
\def \excper {5 }     % Participants excluded because they cannot sort


% Remember to start reftex-mode

\begin{document}

\begin{chapter}{Glossary}
\begin{enumerate}
\item Error-clamp: Refers to the phase of a simulation in the models of motor
adaptation where the error that drives learning is forced to zero. For example,
when performing a task in which a robotic attachment creates forces perpendicular
to the direction of movement, an error-clamp phase is one in which the attachment
pushes the arm constantly towards the desired trajectory, thus making error zero.
\end{enumerate}
\end{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{chapter}{Random ideas}
\section{Straight movements as a sign of learning}
According to Gurevich\_93 (see \cite{Wolpert_Are_1995}) and
\cite{Shadmehr_Adaptive_1994}, the more a person practices a movement, the
straighter this movement becomes. Could we use the curvature of a movement as a
measure of how much they've learned this movement?

This could be useful for the context-dependent learning paradigm. The question
to answer is: if a participant learns context A fully (and their movements
become more straight), when switching back to A, will the first movement be
straight or curved? If it's straight, then we can link this to the idea of
context-based learning.

NOTE: Gurevich\_93 is a doctoral thesis that doesn't seem to be available.


\section{Quick adaptation to perturbations in the irrelevant manifold.}
Models for motor control can be divided into two categories: (1) optimal
trajectory and (2) end point. Optimal trajectory are those in which the
controller tries to stick to a previously-calculated optimal trajectory and any
perturbation (internal, external) will be dealt with by trying to go back to the
optimal trajectory. Endpoint models, on the other hand, do not care that much
about the trajectory itself and, with models that follow the ``minimal
intervention'' principle, will only correct for perturbations if they mess with
the endpoint.

Endpoint models are more recent and arguably better than optimal trajectory
ones. However, there is one thorn on their side: if during a movement an arm is
perturbed, it will immediately try to return to the optimal trajectory
\citep{Bizzi_Posture_1984}, even if the perturbation does not detract from the
endpoint (this last part is my speculation). \cite{Todorov_Optimality_2004}
argues that this is due to impedance, as a sort of immediate dumb fighback.

A nice low-hanging fruit project would be to test whether
\cite{Todorov_Optimality_2004} is right by having participants perform a
reaching task in real life, where a motor creates random perturbations (in the
irrelevant manifold, for good measure) and compare their reactions to the same
task in virtual reality where the perturbation happens in the VR environment,
but not in real time. If participants adapt in the same way in both
environments, then \cite{Todorov_Optimality_2004} is wrong.

\end{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{chapter}{Paper}
In this chapter, I describe the everything related to the paper that should come
out of this project, including modeling and experimental findings, as well as
the direction of the paper and things needed to get there.

\section{Project description}
The idea of the project is the following: We will create an agent that learns to
control an arm in environment A (say, the real world carrying nothing), and
learns how to control the arm in environment B (say, with high winds). The agent
then learns to recognize environments A and B and to deploy the right controller
for each environment depending on this recognition.

We will argue that this is how humans do it and present evidence (based on
literature research) for the predictions this model makes, as well as try to
make new predictions.

After this methods paper is done, model validation can be carried out in future
studies.

\section{Planning}
Here I enumerate the things I need to learn and do in order to get this project
moving and to finish it.

\begin{tabularx}{\textwidth}{
  l|
  >{\hsize=.4\hsize\linewidth=\hsize}X|
  >{\hsize=.2\hsize\linewidth=\hsize}X|
  >{\hsize=.2\hsize\linewidth=\hsize}X}
% {l|{\hsize=.4\hsize}X|{\hsize=0.3\hsize}X|{\hsize=0.3\hsize}X}
End Date & Milestone & A priori comments & A posteriori comments \\ \hline \hline
12.02.2021 & Learn about published models on arm-movement planning, especially those that can be used as generative models & & Discovered low-hanging fruit. Shifting directions a bit. See \ref{subsec:proposal}\\ \hline
19.02.2021 & Learn about measures for how practiced a movement is. Reaction times? Speed profiles? Trajectories? & & \\\hline
12.03.2021 & Implement the best model/s and do some testing as to its capabilities. & Without inference &   \\ \hline
19.03.2021 & Add the sequential goals as another level in the hierarchy. & Still no inference. & \\ \hline
02.04.2021 & Run some exploratory simulations & How well does the model behave? Are there bad cases? & \\ \hline
09.04.2021 & Talk a  bout possible predictions that can be made with the model. & This includes a meeting. & \\ \hline
--- & Run final simulations and write paper. & & \\ \hline

\end{tabularx}


\end{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{chapter}{Modeling}
\section{Random thoughts}
I should read Knill and Pouget 2004 for Bayesian calculations in neurons.

\section{Categorizing models}

\section{Models to implement}
\subsection{Optimal feedback control from \cite{Todorov_Optimal_2002}}
\label{subsec:todorov-2002}
This model, very well specified in the supplementary information of
\cite{Todorov_Optimal_2002}, is an optimal control model, where some cost
function is established and solved alongside the dynamical system.

The model is based on what they call the ``minimal intervention principle'',
where trajectories are only corrected when they interfere with the goals. This
is fancy talk for the idea of using cost functions that are based entirely on
the final goal (and subgoals, if any), as opposed to calculating optimal
trajectories and trying to stick to them. The cost function also must be written
in a way that establishes what is relevant and what isn't. For example, they
talk about a cost function of the form $x_1 + x_2 - x*$, which does not specify
what the end positions of $x_1$ and $x_2$ have to be, but only that their sum
must be close to $x*$.

Once the cost function is created, the system of differential equations (see
below), cost function and observation feedback functions are all solved by
casting the problem in terms of a Kalman filter.

It seems like the model is ready to be applied as-is, with modifications only at
the level of the dynamical system and the cost function.

From \cite{Wolpert_Principles_2011}:
\begin{enumerate}
\item ``The minimum intervention principle has now
been demonstrated in a number of tasks including the seemingly simple task of
generating a target force with the tip of the index finger[35]''
\item ``Recently, it has been pro- posed that such impedance control can be
brought within an optimal control framework by formulating impedance control as
the optimal response in the face of uncertainty about the dynamics of the body
and environment[39].''
\end{enumerate}

\subsection{Adaptive dynamics from \cite{Shadmehr_Adaptive_1994}}
\label{subsec:shadmehr-1994}
The gist of this model is as follows: first, determine an optimal trajectory
$q^*(t)$, then set up the controller of each joint such that $q^*(t)$ is an
attractor (for $0 \le t \le T$) of the system and then just let it rip. The
optimal trajectory in this paper is obtained using the minimum jerk model
\citep{Hogan_organizing_1984}.

Because of the use of an optimal trajectory that the controlled system is trying
to stick to, this model differs from that presented by
\cite{Todorov_Optimal_2002} (see \ref{subsec:todorov-2002}). In addition to this
essential difference, this model also does not have the possibility of feedback
signals (visual or proprioceptive) and, due to its complexity, is probably less
viable as a generative model for inversion, though that might be a problem for
future studies and not this paper.

\subsection{Object avoidance from \cite{Hamilton_Controlling_2002}}
In this paper they're touting the awesomeness of the TOPS model
\citep{Harris_Signaldependent_1998} and applying it to a reaching problem with
an obstacle in between.

The implementation of this model is way too handcrafted. The gist of it is as
follows: They put an obstacle between the starting and ending positions and
assume that the optimal trajectory is a polynomial of fifth order (maybe even
just $y = Ax^5$, I'm not sure). To optimize its parameters they created a cost
function with two parts: the first part is the predicted covariance of the end
point and the second part is a term penalizing ``visiting'' the obstacle. They
basically discard any trajectory that gets close to the obstacle.

While it works, I find it too ad hoc to be useful.


\subsection{2-timescale motor learning from \cite{Lee_Dual_2009}}
\label{subsec:lee-dual}
This is a motor learning model, i.e. a model for how we can learn to perform
movements in an environment that is different from the default. For example,
under strong winds. It's an extension of the model presented by
\cite{Smith_Interacting_2006} to the problem of ``dual adaptation'', i.e. the
adaptation to different contexts and their force fields, and switching between
them.

While they present many models for dual adaptation, through model comparison (of
the qualitative type) they arrive at the conclusion that the only good one is
the parallel 1-fast N-slow model, which I describe here.

Consider an abstract motor system (for example, an arm) that has learned to
perform its movements in a baseline environment. Let $y(t) \in \mathbb{R}$ be
the outcome for some motor signal given at time $t$ by the controller in the
baseline environment. Now, assume that in a new environment, with some force
field that is unknown to the controller, the controller issues the command that
under the baseline environment would lead to $y(t)$, but, due to the force
field, it leads to the outcome $f(t)$. Now, define a prediction error between
the expected outcome and the observed outcome as $e(t) = f(t) - y(t)$. The model
from \cite{Lee_Dual_2009} postulates two adaptation systems, one fast and one
slow, which learn from this prediction error.

Both slow and fast systems adapt with a similar rule:
\[
x_{f/s}(t + 1) = A_{f/s}x_{f/s}(t) + B_{f/s}e(n)c(n)^T \label{eqn:lee-learn}
\]
where $x$ is a ``learning process'', $f/s$ refers to fast/slow (one equation
each), and $A$ and $B$ are learning rates. $c(n)$ is the environment variable
that informs the controller which environment is currently active.
$x_s,c, B \in \mathbb{R}^M$ to account for multiple environments. In the case
where the learning of one environment does not interfere with the learning of
others (as in \cite{Lee_Dual_2009}), $c = (0, ..., 1, 0, ..., 0)$, where the 1
is in the position of the current environment. Note that the fast process is
single-dimensional in this model (see \citep{Lee_Dual_2009} for the reasoning).

The controller is then given by:
\[
y(t) = x_f(t) + x_s(t)c(t)^T
\]

\subsection{Proposal: Bayesian motor learner}
\label{subsec:proposal}
This model is a Bayesian alternative to the N-state learners from
\cite{Lee_Dual_2009}. In this model, there is no need for explicit different
timescales and multiple states; instead, the model relies on Bayesian learning
of the best control, combined with inference over the current context. This
model will, hopefully, reproduce all the experimental findings from the
literature on motor learning (e.g. saving, interference).

The model consists of three parts: (1) context inference, (2) decision making,
(3) updating of the decision-making rules.

Before describing these parts, I present some nomenclature:
\begin{itemize}
\item $t$: Time at the moment an observation is made, before inference is done.
\item $\theta_t$: Observation. Note that this is assumed to be a noisy
observation of the generalized state (see below) given by $\theta_t = h(z_t)$.
\item $s_t$: Hidden state, e.g. the position of the hand. It is assumed not to
depend on context. E.g. the physical position of the hand.
\item $a_t$: Action taken after observing $\theta_t$ and inferring the context.
\item $c_t$: Context inferred after observing $\theta_t$.
\item $\omega_t$: Contextual cue.
\item $z_t$: Generalized state, $z_t = \{s_t, c_t, \omega_t\}$. Observed noisily
as $\theta_t$.
\end{itemize}

Context inference can work through context-specific (possibly unreliable) cues,
or through the dynamics themselves, using prediction error as the force behind
inference. Let us assume that both parts are present in the observation
$\theta_t$. In this case, the inferred generalized state is given by
\[
  q(z_t | \theta_t, \theta_{t-1}, a_{t-1}) \propto p(\theta_t | z_t)p(z_t|\theta_{t-1}, a_{t-1})
\]
  
Additionally, after having inferred the current state, motor adaptation is given by
\[
q(\gamma | z_t, z_{t-1}, a_{t-1}) \propto p(z_t | z_{t-1}, a_{t-1}, \gamma)p(z_{t-1})p(\gamma)
\]

where $\gamma$ are the parameters of the internal model for the dynamics,
i.e. the function $f: (z_t, a_t) \rightarrow z_{t+1}$, which determines the beliefs of
the agent as to how the system evolves after having taken action $a_t$.

Having inferred the generalized state (including context), a decision is made
sampling from $p(a_t | z_t, \beta)$, which is a distribution that is assumed to
be parametrized by $\beta$. These parameters can be updated via:
\[
q(\beta | z_t, \theta_{t+1}, a_t) \propto p(\theta_{t+1} | \beta, z_t,
a_t)p(\beta) \label{eqn:update-parameters}
\]

\subsubsection{Expected results}
I predict that this model will display all the experimental findings that other
models do, without invoking a possibly-infinite hierarchy of time scales, each
with their own learning rate. Here I briefly go through the experimental
findings and why I expect the model to reproduce them.

Savings. This refers to the effect that, while learning a new context, if
learning is interrupted by going back to a baseline context (e.g. no force
field) and the new context is later retaken, the new dynamics do not need to be
learned from scratch. Instead, learning continues where it was left off, with
the possibility of some performance having been lost (i.e. learning having been
forgotten). The Bayesian lerner displays this behavior simply by virtue of the
learning of contex-specific action rules (learned dynamics), which are ``stored
away'' once the context is no longer the new one (i.e. it has returned to
baseline).

Retrograde interference. This refers to the learning of context A interfering
with the learning of context B. This occurs, for example, if context A has a
force field that pushes right, while context B has a force field that pushes
left. In this case, learning B is made slower by having learned Abefore,
compared to learning B without having encountered A. This effect will be
observed in the behavior of the model if context inference needs to accumulate
before the agent is sure that a context switch has occured. In the meantime,
context A's learned dynamics will interfere with context B's because actions are
taken with a rule that depends on context (via $z_t$). Related to this, the
model makes three predictions: (1) Contextual cues (that can be trusted) will
reduce interference. (2) The amount of learning on contexts A and B will affect
intereference such that highly trained contexts will interfere more
strongly. (3) The duration of interference will be reduced in the case of
highly-learned, opposing contexts, as the prediction error of the outcomes of
actions will quickly drive contexts to switch to the current one.

Rapid de-adaptation. This is the phenomenon in which forgetting an adaptation is
faster than learning it. This would happen simply through context-based
learning, where getting out of a context means that adaptations are no longer
used (and eventually are forgotten). Note: I need to read more on this
phenomenon, as I might be misinterpreting what it means.

Spontaneous recovery. In an A-B-error\_clamp paradigm, in which during B the
opposite adaptation to A is learned, at the beginning of the error-clamp phase
(after B) participants seem to quickly gravitate towards A (thinking that A is
now active?). This would be displayed by the model via the state inference, as
now the visual cue and the observed dynamics point to different contexts. In
time, updating $p(context | visual cue)$ will lead to spontaneous recovery.

Bicycle effect. We never forget how to ride a bicycle. Existing models would
need to incorporate an almost-infinite hierarchy of time scales to account for
this, but my Bayesian learner naturally accounts for it by making use of
probability distributions: the longer you practice something, the more
entrenched the adaptation becomes by virtue of the posterior probability
becoming very peaky.


\end{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{chapter}{Behavioral findings, Physiology and neurons}
A few notes from \cite{Wolpert_Are_1995}:
\begin{enumerate}
\item Hand paths become straight with practice (Gurevich 93, Shadmehr and
Musa-Ivaldi 94)
\item Deafferented patients move differently (Ghez 90)
\item Bell-shaped speed emerges from dynamics of control, not from planning
(Jordan 94)
\item Curvature matches the participant's messed up perception of straight lines
(wolpert 94)

\end{enumerate}

\section{Notes from \cite{Harris_Signaldependent_1998}}
\begin{enumerate}
\item Arm stiffness can be adapted (Burdet et al 01.
\end{enumerate}

\section{Notes from \cite{Bays_Computational_2007}}
\begin{enumerate}
\item We have the ability to recall previously learned dynamics even after
months! (Bashers-Krug et al 96, Gandolfo et al 96).
\end{enumerate}

\section{Notes from \cite{Shadmehr_Adaptive_1994}}
\begin{enumerate}
\item When planning movement, the target is transformed from a retinocentric
vector to a head-centered and finally a shoulder-centered one. (Andersen\_85,
Soechting and Flanders\_91).
\item Gordon\_93 says that the target is finally represented as a vector from
the current hand positoin (or whatever the effector is) to the goal position.
\item From Lackner\_and\_Dizio\_92, aftereffects exist when introduced Coriolis
forces are withdrawn. They also exist when perception is mucked with with prism
glasses (Held\_59, 62, 63).
\end{enumerate}


\section{Notes from \cite{deC.Hamilton_scaling_2004}}
\begin{enumerate}
\item ``Muscle activity in specific tasks shows stereotypy whether subjects
generate force with the fingertips (Valero-Cuevas et al. 1998), the wrist
(Hoffman and Strick 1999), the neck (Vasavada et al. 2002) or the arm (van
Zuylen et al. 1988; Flanders and Soechting 1990; Buchanan et al. 1993; van
Bolhuis and Gielen 1997).''
\item ``Primates also show repeatable patterns of muscle activation when
grasping the same object repeatedly, and different activation patterns for
different objects (Brochier et al. 2001). This suggests that stereotypy is a
general characteristic of movement and is not unique to humans.''
\item There is a paragraph that discusses different cost functions. For example,
one is based on the total torque, another on fatigue and endurance, and other
mathematical alchemy. Lots of references there too.

\end{enumerate}

\section{Notes from \cite{Kojima_Memory_2004}}
\begin{enumerate}
\item ``Previous studies have shown that adaptation is relatively specific to
the size and direction of target eccentricity (Deubel et al., 1987; Semmlow et
al., 1989; Frens and van Opstal, 1994; Straube et al., 1997; Noto et al., 1999;
Watanabe et al., 2000).''
\item ``It has been demonstrated that adaptation progresses more slowly when the
subject is exposed to adapting target steps of a variety of sizes and directions
in humans (Miller et al., 1981) and monkeys (Scudder et al., 1998).''
\end{enumerate}


\section{Notes from \cite{Lee_Dual_2009}}
``When given contextual cues and sufficient trials, humans can simultaneously
adapt to two opposite force fields (Osu et al., 2004; Nozaki et al., 2006;
Howard et al., 2008), two saccadic gains (Shelhamer et al., 2005), or several
visuomotor rotations (Imamizu et al., 2007; Choi et al., 2008).''

\section{Motor adaptation}
Motor adaptation to modified environments (e.g. force fields) has been shown to
display the following characteristics:
\begin{enumerate}
\item Savings: Learning a previously-learned environment (after some extinction
phase) is faster than the original learning. This means that the previously
learned model is saved for a while and its learning can be resumed after
extinction. However, \cite{Kojima_Memory_2004} showed that savings can be
``washed out'' if baseline trials (no force field) are inserted between
extinction and the second learning phase.
\item Anterograde interference: Learning an adaptation (e.g. force field
pointing left) slows down learning an opposing adaptation (e.g. force field
pointing right).
\item Rapid de-adaptation: Unlearning an adaptation (even after fully learning
it) is faster than tlearning it \citep{Davidson_Scaling_2004}
\end{enumerate}

\end{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{chapter}{Notes on papers}
Conclusions and useful things about different papers I read. This differs from
the comments above in that the notes here are holistic.

\section{\cite{Shadmehr_Adaptive_1994}}
In this paper they show a number of things. First, the show that people can
adapt the dynamics of their control states to match changing dynamics of the
observers. They also show that this adaptation takes the form of a straightening
of trajectories, which are very twisted when they are first introduced to new
system dynamics (viscosity of the environment); participants learn to do
straight lines in the new environment after some trials, and the learing is
monotonically increasing. They show a model of the dynamics of the arm, the
control states and the adaptation that happens when exposed to the new
dynamics. Finally, they show, both experimentally and in their model, the
aftereffects of the new dynamics, whereby they display bad behavior when they go
back to the natural environment, but eventually adapt to it as well.

Interesting paper that cound be the foundation of our models, assuming that it
hasn't been improved upon and maybe entirely replaced by newer stuff.

A description of their model can be seen in \ref{subsec:shadmehr-1994} above.

\section{\cite{Todorov_Optimal_2002}}
See \ref{subsec:todorov-2002}

\section{\cite{deC.Hamilton_scaling_2004}}
The authors make use of the TOPS strategy \citep{Harris_Signaldependent_1998}
and an experiment where they measure torque at different joints to show that
bigger muscles produce less variability than smaller muscles for the same level
of voluntary torque (I wonder what the opposite of voluntary is).

It's an experimental paper with some muscle simulations thrown in for good
measure. Contains a lot of references that might be useful later on the nature
and characteristics of human and primate movements, as well as cost functions
used in the past for modeling them.

\section{\cite{Beers_When_2002}}
This paper speaks of a model of optimal adaptation and weighting of visual and
proprioceptive cues. Nothing of interest for us as of now.

\section{\cite{Scheidt_Learning_2001}}
Humans can adapt their movements to stochasticly varying changes in the dynamics
of the environment. Moreover, they showed that their adaptation is most
complatible with the idea that participants adapt to the mean of the dynamics (a
one-directional force of changing magnitude) of the environment instead of doing
trial-to-trial adaptations.

These views, however, are challenged by the two-time-scales models of learning,
like \cite{Smith_Interacting_2006}.

\section{\cite{DeKleijn_Everyday_2014}}
They cite \cite{Henry_Increased_1960} saying that participants have longer
reaction times when preparing complex movements than when preparing simple
ones. May be worth checking out.

\section{\cite{Haar_Motor_2020}}
This is the only paper I know where they do motor learning in a very natural
task: playing billiards. It's great and I wish I could do this research.

They tracked the joints of participants who were doing the same pool shot over
and over (300 times). I do not think there was any variation in initial
positions. They track performance and variability in joint movement.

\section{\cite{Wolpert_Principles_2011}}
Good review on motor learning. many relevant citations here.

\section{\cite{Todorov_Optimality_2004}}
Review of optimal control. Citation goldmine! Lots of papers on optimal control
in biological systems are cited here.

In particular, [43] and [44] might be similar to the idea of inferring goals and
predicting future movements.

``However, feedback control[25] explains an important additional observation:
the increased duration of more accurate movements is due to a prolonged
deceleration phase, making the speed profiles significantly skewed[88].''

\end{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{chapter}{Implementing the adaptation model} In this section, I propose a
simulated task in which the prowess of the model proposed in
\ref{subsec:proposal} is put to the test. I additionally present simulation
results and some writeup. Basically a mini-paper on this model.

\section{The Task} The tasks used for models like this one are usually
\citep[e.g.][]{Lee_Dual_2009} reaching tasks: a participant sits in front of a
table with shoulders immobilized, and has to bring her hand from the starting
position A to a designated position B. This is done while holding a robotic arm
that moves with the hand. The robotic arm is used to create force fields in
different directions (CW and CCW, typically, but it's basically left and right)
for the different contexts. The position of the participant's arm is recorded
and deviations from the straight line are considered as errors ($e(t)$ in
\ref{eqn:lee-learn}).

To simplify the task, I propose the following: participants need to keep their
hand still in the starting position for the duration of the trial. The robotic
arm creates forces that will drive the arm away from its current position by
pushing in a constant direction with constant force. This experiment has the
same elements as the one by \cite{Lee_Dual_2009} but eliminates the necessity
of an underlying movement. While it works wonders as a thought experiment and
helps me see if the proposed model behaves the way I want, it must not be used
as a real experiment because participants could simply use impedance for each
trial, regardless of context.

In this experiment, every context $C_i$ is characterized by the strength and
direction of its force: $C_i = \{\theta_i, \gamma_i\}$, where $i = 1, 2, ...,
N_c$, $N_c$ is the number of contexts, $\theta$ is the angle of the force
measured Cartesianly, and $w_i$ is the intensity of the force.

As in \cite{Lee_Dual_2009}, I will simulate an A-B-clamp paradigm, which starts
with a context $C_0 = \{0, 0\}$ in which no force is being applied. Then
context $C_A$ is shown for a few trials, then context $C_B$, then an
error-clamp block in which the hand is forced to stay at the initial position
by the robotic arm.

Additionally, a contextual cue $\omega$ is shown to the participant which
signals the current context. The experiment will be run with different levels
of reliability of this cue. Different sessions will be run with different cue
reliabilities.

An experimental session $\Xi$ is characterized by the reliability of the cue,
the contexts avaiable and the order in which they are shown. In a given
session, each trial begins with the cue and the XXX seconds during which the
participant must keep the hand in the starting position. During the intertrial
time, no forces are applied to the hand, but the hand must remain in the
starting position.

\section{The model}
The model is an implementation of \ref{subsec:proposal}. The generalized state
is given by:
\begin{equation}
z_t = \{x, \theta_t, \gamma_t, \omega_t\}
\end{equation}
where $x$ is the current position of the hand in Cartesian coordinates,
where the origin is the starting point. We assume that motor commands are
issued every $\Delta t$ for simplicity. At the beginning of each time interval,
the context is inferred combining the cue and the prediction error of the
outcome of the previous motor command. We assume further than the cue and the
force field are uncorrelated, which yields:
\begin{align}
  q(C_t) &= q(C_t | \omega_t)q(C_t | s_t, s_{t-1}, a_{t-1}) \\ \label{eqn:estimated-context}
  q(C_t | \omega_t) &\propto p(\omega_t | C_t)p(C_t) \\
  q(C_t | x_t, x_{t-1}, a_{t-1}) &\propto p(x_t | C_t, x_{t-1}, a_{t-1})p(x_t)p(x_{t-1})p(C_t)
\end{align}
The terms $p(C_t)$ refer to the prior probability of the context at
the beginning of the trial: at the first trial, this refers to prior beliefs
over which contexts are more common/likely. At each subsequent trial, it
incorporates the belief that has so far been accumulated, given previous
trials.

Once the posterior over contexts has been calculated for the current trial, it
is used to select the current context. A simple rule is to sample the context
directly from the posterior distribution. However, a useful extension of this
is to use a softmax function to create a distribution:

\begin{equation}
S(C^{(t)}) = \frac{e^{\kappa
q\left(C^{(t)}\right)}}{\displaystyle \sum_{i=1}^{N_c}e^{\kappa
q\left(C_i\right)}}
\end{equation}
where $q(C_i)$ is given by \eref{eqn:estimated-context}. The
function $S(\cdot)$ is then used to sample the (inferred) current context. This
representation has the advantage of adding a parameter with which the sampling
behavior can be characterized, ranging from choosing uniformly when $\kappa =
0$ to choosing the context with the highest probability deterministically as
$\kappa \rightarrow \infty$.

In the case where the hand position can be determined by the participant with
perfect accuracy (ignoring visual and proprioceptive error), we have that
$p(x_\tau) = \delta(x_\tau, \hat x)$, where $\hat x$ is the real position of
the hand, and $\delta (\cdot, \cdot)$ is the Kronecker delta function. We
assume this to be the case during this experiment.

For simplicity, we assume that participants know how many distinct contexts
will be shown to them, and therefore all the $C_x$ have been created by the
agent before the session begins. By default, all contexts assume a force with
angle and magnitude equal zero, and their true values are learned during the
experiment (see below).

For mathematical tractability (i.e. to avoid circular distributions), we will
limit contexts to one where the force points left (angle $\pi$) and one where
the force points right (angle 0), in addition to the baseline context. In this
scheme, participants need only learn the magnitude of the force for each
context. We will assume that for each context, the agent's belief over the
magnitude of the force is given by a normal distribution:
\begin{equation}
p(\gamma) = N(\mu_f, \sigma_f) \label{eqn:data-dist}
\end{equation}
where $\mu_f$ and $\sigma_f$ are parameters to be estimated at each
trial. Before the experiment begins, the agent will have a prior distribution
over these parameters. A standard Bayesian approach is to choose NormalGamma
priors:
\begin{equation}
p(\mu_f, \sigma_f) = NG(\mu_0, \nu_0, \alpha_0, \beta_0)
\end{equation}
where $\mu_0, \nu_0, \alpha_0$ and $\beta_0$ are free hyperparameters of the
model. Because participants are told that there are three contexts (baseline,
left and right), we assume the following hyperparameter values:
\begin{align} \mu_f^{(0)} &= (0, -1, 1) \\
  \nu_f^{(0)} &= 0.1 \\
  \alpha_f^{(0)} &= 1 \\
  \beta_f^{(0)} &= 0.5
\end{align}
Note that $\mu_f^{(0)}$ is different for each context and is thus
provided as a vector $(0, -1, 1)$ representing the baseline, left and right
contexts, respectively. For all other hyperparameters, all contexts have the
same value. These values ensure that the prior over the hyperparameters
$(\mu_f, \sigma_f)$ has the mode at $(\mu_f^{(0)}, 1)$. We will show that the
choice of these hyperparameters affects learning rate, but not the final
learned magnitude \todo{Check this}.

The update equations for the magnitud parameters are given by:
\begin{equation}
q(\mu_f, \sigma_f | x_t, x_{t-1}, a_{t-1}) \propto p(x_t |
x_{t-1}, a_{t-1}, C_t)p(\mu_f, \sigma_f) \label{eqn:context-from-x}
\end{equation}
where $p(x_t | x_{t-1}, a_{t-1}, C_t)$ is a Gaussian distribution centered
around $\mu_a$ with standard deviation $\sigma_a$, which is a free parameter of
the model. It is assumed that the standard deviation over outcomes ($\sigma_a$)
is related to that of the parameter $\mu_f$ as follows:
\begin{equation}
\sigma_f = \sigma_a / \nu_f
\end{equation}
which makes $\nu_f$ a parameter to estimate. Because we chose the priors to be
NormalGamma and the likelihood to be Gaussian, the posteriors are also
NormalGamma, and the parameters are updated after one time step via:
\begin{align}
  \mu_f^{(t)} &= \frac{\nu_f^{(t-1)} \mu_f^{(t-1)} + x_t}{\nu_f + 1} \\
  \nu_f^{(t)} &= \nu_f^{(t-1)} + 1 \\
  \alpha_f^{(t)} &= \alpha_f^{(t-1)} + 0.5 \\
  \beta_f^{(t)} &= \beta_f^{(t-1)} + \frac{\nu_f^{(t-1)}}{\nu_f^{(t-1)} +
                  1}\frac{\left(x - \mu_f^{(t-1)}\right)^2}{2}
\end{align}


The final step is action selection. Action selection is done at the level of
desired outcome (as opposed to motor commands) following the equations:
\begin{align}
  x_{t+1} &= x_t + a_t + f_t + \sqrt{\Delta
            t}\epsilon\\ \label{eqn:dynamics}
  f_t &= \gamma_t (cos\theta_t, sin\theta_t) \\
  a_t &= \argmax_{a_t}p(x_{t+1} = (0, 0) | x_t, a_t, f_t)
\end{align}
where $\gamma_t$ and $\theta_t$ are the participant's estimate of
the parameters, given by \eref{eqn:estimated-context}. $\epsilon$ is a Gaussian
motor error component.

\section{Simulation results}

\section{Conclusions}



\end{chapter}






\bibliography{../MyLibrary}

\end{document}

