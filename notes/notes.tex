\documentclass{report}
\usepackage{graphicx}
\usepackage{breakcites}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tabularx}
\usepackage[margin=1.5cm]{geometry}
\usepackage{todonotes}
\bibliographystyle{apalike}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

% Model and task parameter macros
\input{parameters}


% Defining parameters
\def \parts {60 }                  % Number of participants
\def \fref #1{Figure \ref{#1}}     % Reference figures
\def \tref #1{Table \ref{#1}}      % Reference tables
\def \cents {20 }                  % Cents earned per mini-block
\def \eref #1{Equation \ref{#1}}   % Reference equations
\def \sref #1{Section '\nameref{#1}'}    % Reference sections
\def \supmat {the Supplementary Materials}
\def \excper {5 }     % Participants excluded because they cannot sort


% Remember to start reftex-mode

\begin{document}

\begin{chapter}{Glossary}
\begin{enumerate}
\item Error-clamp: Refers to the phase of a simulation in the models of motor
adaptation where the error that drives learning is forced to zero. For example,
when performing a task in which a robotic attachment creates forces perpendicular
to the direction of movement, an error-clamp phase is one in which the attachment
pushes the arm constantly towards the desired trajectory, thus making error zero.
\end{enumerate}
\end{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{chapter}{Random ideas}
\section{Straight movements as a sign of learning}
According to Gurevich\_93 (see \cite{Wolpert_Are_1995}) and
\cite{Shadmehr_Adaptive_1994}, the more a person practices a movement, the
straighter this movement becomes. Could we use the curvature of a movement as a
measure of how much they've learned this movement?

This could be useful for the context-dependent learning paradigm. The question
to answer is: if a participant learns context A fully (and their movements
become more straight), when switching back to A, will the first movement be
straight or curved? If it's straight, then we can link this to the idea of
context-based learning.

NOTE: Gurevich\_93 is a doctoral thesis that doesn't seem to be available.


\section{Quick adaptation to perturbations in the irrelevant manifold.}
Models for motor control can be divided into two categories: (1) optimal
trajectory and (2) end point. Optimal trajectory are those in which the
controller tries to stick to a previously-calculated optimal trajectory and any
perturbation (internal, external) will be dealt with by trying to go back to the
optimal trajectory. Endpoint models, on the other hand, do not care that much
about the trajectory itself and, with models that follow the ``minimal
intervention'' principle, will only correct for perturbations if they mess with
the endpoint.

Endpoint models are more recent and arguably better than optimal trajectory
ones. However, there is one thorn on their side: if during a movement an arm is
perturbed, it will immediately try to return to the optimal trajectory
\citep{Bizzi_Posture_1984}, even if the perturbation does not detract from the
endpoint (this last part is my speculation). \cite{Todorov_Optimality_2004}
argues that this is due to impedance, as a sort of immediate dumb fighback.

A nice low-hanging fruit project would be to test whether
\cite{Todorov_Optimality_2004} is right by having participants perform a
reaching task in real life, where a motor creates random perturbations (in the
irrelevant manifold, for good measure) and compare their reactions to the same
task in virtual reality where the perturbation happens in the VR environment,
but not in real time. If participants adapt in the same way in both
environments, then \cite{Todorov_Optimality_2004} is wrong.

\end{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{chapter}{Paper}
In this chapter, I describe the everything related to the paper that should come
out of this project, including modeling and experimental findings, as well as
the direction of the paper and things needed to get there.

\section{Project description}
The idea of the project is the following: We will create an agent that learns to
control an arm in environment A (say, the real world carrying nothing), and
learns how to control the arm in environment B (say, with high winds). The agent
then learns to recognize environments A and B and to deploy the right controller
for each environment depending on this recognition.

We will argue that this is how humans do it and present evidence (based on
literature research) for the predictions this model makes, as well as try to
make new predictions.

After this methods paper is done, model validation can be carried out in future
studies.

\section{Planning}
Here I enumerate the things I need to learn and do in order to get this project
moving and to finish it.

\begin{tabularx}{\textwidth}{
  l|
  >{\hsize=.4\hsize\linewidth=\hsize}X|
  >{\hsize=.2\hsize\linewidth=\hsize}X|
  >{\hsize=.2\hsize\linewidth=\hsize}X}
% {l|{\hsize=.4\hsize}X|{\hsize=0.3\hsize}X|{\hsize=0.3\hsize}X}
End Date & Milestone & A priori comments & A posteriori comments \\ \hline \hline
12.02.2021 & Learn about published models on arm-movement planning, especially those that can be used as generative models & & Discovered low-hanging fruit. Shifting directions a bit. See \ref{subsec:proposal}\\ \hline
19.02.2021 & Learn about measures for how practiced a movement is. Reaction times? Speed profiles? Trajectories? & & \\\hline
12.03.2021 & Implement the best model/s and do some testing as to its capabilities. & Without inference &   \\ \hline
19.03.2021 & Add the sequential goals as another level in the hierarchy. & Still no inference. & \\ \hline
02.04.2021 & Run some exploratory simulations & How well does the model behave? Are there bad cases? & \\ \hline
09.04.2021 & Talk a  bout possible predictions that can be made with the model. & This includes a meeting. & \\ \hline
--- & Run final simulations and write paper. & & \\ \hline

\end{tabularx}


The previous time table was interrupted by the possibility of the new project. Now that this new project has taken shape, this new time table applies:

\begin{tabularx}{\textwidth}{
  l|
  >{\hsize=0.4\hsize\linewidth=\hsize}X|
  >{\hsize=0.2\hsize\linewidth=\hsize}X|
  >{\hsize=0.2\hsize\linewidth=\hsize}X}
End Date & Result & A priori comments & A posteriori comments \\ \hline \hline
30.04.2021 & Lit. research on savings and deadaptation & Are there many with contextual cues? & I found a lot of evidence. Moving on.\\ \hline
07.05.2021 & Finding model pars to explain observations & This is mostly already done &  Parameters found\\\hline
14.05.2021 & Looking into error-clamp behavior & Maybe there's something to it. If there is, an extra week should be allocated to simulations. & I found a lot of evidence for context-inference-based motor adaptation. It will be incorporated into the paper. \\\hline
23.06.2021 & Looking into single-cell recordings. & Perhaps I can find evidence here. & Evidence seems dubiuous, but might be worth including in Discussion \\
--- & Start writing the paper. & & This was delayed by the defense and dysco. Restarted around 24.06.2021 \\ \hline

\end{tabularx}

\section{Results}
Possible results to include in the paper:
\begin{enumerate}
\item Quick de-adaptation/savings: gradual evidence accumulation during context inference will make the switching between internal models not be immediate.
\item Learning: The speed in which the internal models are adjusted given errors depends on how certain the inference is. More uncertain context inference will lead to slower learning.
\item Action selection: Action selection is sampled from the existing (relevant) internal models, weighted by their posterior probability during this trial.
\item Cue reliability adjustments: As more bad-cue trials are observed, the agent will lower the estimated reliability of the cue.
\item Error-clamp tappering off: Hundreds of error-clamp trials make adaptation seem to slowly disappear, but it never fully goes away.
\item Error-clamp lag: For a while, the tappering off does not begin, depending on how clear it is that error-clamp trials have started.
\end{enumerate}




\end{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{chapter}{Modeling}
\section{Random thoughts}
I should read Knill and Pouget 2004 for Bayesian calculations in neurons.

\section{Categorizing models}

\section{Models to implement}
\subsection{Optimal feedback control from \cite{Todorov_Optimal_2002}}
\label{subsec:todorov-2002}
This model, very well specified in the supplementary information of
\cite{Todorov_Optimal_2002}, is an optimal control model, where some cost
function is established and solved alongside the dynamical system.

The model is based on what they call the ``minimal intervention principle'',
where trajectories are only corrected when they interfere with the goals. This
is fancy talk for the idea of using cost functions that are based entirely on
the final goal (and subgoals, if any), as opposed to calculating optimal
trajectories and trying to stick to them. The cost function also must be written
in a way that establishes what is relevant and what isn't. For example, they
talk about a cost function of the form $x_1 + x_2 - x*$, which does not specify
what the end positions of $x_1$ and $x_2$ have to be, but only that their sum
must be close to $x*$.

Once the cost function is created, the system of differential equations (see
below), cost function and observation feedback functions are all solved by
casting the problem in terms of a Kalman filter.

It seems like the model is ready to be applied as-is, with modifications only at
the level of the dynamical system and the cost function.

From \cite{Wolpert_Principles_2011}:
\begin{enumerate}
\item ``The minimum intervention principle has now
been demonstrated in a number of tasks including the seemingly simple task of
generating a target force with the tip of the index finger[35]''
\item ``Recently, it has been pro- posed that such impedance control can be
brought within an optimal control framework by formulating impedance control as
the optimal response in the face of uncertainty about the dynamics of the body
and environment[39].''
\end{enumerate}

\subsection{Adaptive dynamics from \cite{Shadmehr_Adaptive_1994}}
\label{subsec:shadmehr-1994}
The gist of this model is as follows: first, determine an optimal trajectory
$q^*(t)$, then set up the controller of each joint such that $q^*(t)$ is an
attractor (for $0 \le t \le T$) of the system and then just let it rip. The
optimal trajectory in this paper is obtained using the minimum jerk model
\citep{Hogan_organizing_1984}.

Because of the use of an optimal trajectory that the controlled system is trying
to stick to, this model differs from that presented by
\cite{Todorov_Optimal_2002} (see \ref{subsec:todorov-2002}). In addition to this
essential difference, this model also does not have the possibility of feedback
signals (visual or proprioceptive) and, due to its complexity, is probably less
viable as a generative model for inversion, though that might be a problem for
future studies and not this paper.

\subsection{Object avoidance from \cite{Hamilton_Controlling_2002}}
In this paper they're touting the awesomeness of the TOPS model
\citep{Harris_Signaldependent_1998} and applying it to a reaching problem with
an obstacle in between.

The implementation of this model is way too handcrafted. The gist of it is as
follows: They put an obstacle between the starting and ending positions and
assume that the optimal trajectory is a polynomial of fifth order (maybe even
just $y = Ax^5$, I'm not sure). To optimize its parameters they created a cost
function with two parts: the first part is the predicted covariance of the end
point and the second part is a term penalizing ``visiting'' the obstacle. They
basically discard any trajectory that gets close to the obstacle.

While it works, I find it too ad hoc to be useful.


\subsection{2-timescale motor learning from \cite{Lee_Dual_2009}}
\label{subsec:lee-dual}
This is a motor learning model, i.e. a model for how we can learn to perform
movements in an environment that is different from the default. For example,
under strong winds. It's an extension of the model presented by
\cite{Smith_Interacting_2006} to the problem of ``dual adaptation'', i.e. the
adaptation to different contexts and their force fields, and switching between
them.

While they present many models for dual adaptation, through model comparison (of
the qualitative type) they arrive at the conclusion that the only good one is
the parallel 1-fast N-slow model, which I describe here.

Consider an abstract motor system (for example, an arm) that has learned to
perform its movements in a baseline environment. Let $y(t) \in \mathbb{R}$ be
the outcome for some motor signal given at time $t$ by the controller in the
baseline environment. Now, assume that in a new environment, with some force
field that is unknown to the controller, the controller issues the command that
under the baseline environment would lead to $y(t)$, but, due to the force
field, it leads to the outcome $f(t)$. Now, define a prediction error between
the expected outcome and the observed outcome as $e(t) = f(t) - y(t)$. The model
from \cite{Lee_Dual_2009} postulates two adaptation systems, one fast and one
slow, which learn from this prediction error.

Both slow and fast systems adapt with a similar rule:
\[
x_{f/s}(t + 1) = A_{f/s}x_{f/s}(t) + B_{f/s}e(n)c(n)^T \label{eqn:lee-learn}
\]
where $x$ is a ``learning process'', $f/s$ refers to fast/slow (one equation
each), and $A$ and $B$ are learning rates. $c(n)$ is the environment variable
that informs the controller which environment is currently active.
$x_s,c, B \in \mathbb{R}^M$ to account for multiple environments. In the case
where the learning of one environment does not interfere with the learning of
others (as in \cite{Lee_Dual_2009}), $c = (0, ..., 1, 0, ..., 0)$, where the 1
is in the position of the current environment. Note that the fast process is
single-dimensional in this model (see \citep{Lee_Dual_2009} for the reasoning).

The controller is then given by:
\[
y(t) = x_f(t) + x_s(t)c(t)^T
\]

\subsection{Proposal: Bayesian motor learner}
\label{subsec:proposal}
This model is a Bayesian alternative to the N-state learners from
\cite{Lee_Dual_2009}. In this model, there is no need for explicit different
timescales and multiple states; instead, the model relies on Bayesian learning
of the best control, combined with inference over the current context. This
model will, hopefully, reproduce all the experimental findings from the
literature on motor learning (e.g. saving, interference).

The model consists of three parts: (1) context inference, (2) decision making,
(3) updating of the decision-making rules.

Before describing these parts, I present some nomenclature:
\begin{itemize}
\item $t$: Time at the moment an observation is made, before inference is done.
\item $\theta_t$: Observation. Note that this is assumed to be a noisy
observation of the generalized state (see below) given by $\theta_t = h(z_t)$.
\item $s_t$: Hidden state, e.g. the position of the hand. It is assumed not to
depend on context. E.g. the physical position of the hand.
\item $a_t$: Action taken after observing $\theta_t$ and inferring the context.
\item $c_t$: Context inferred after observing $\theta_t$.
\item $\omega_t$: Contextual cue.
\item $z_t$: Generalized state, $z_t = \{s_t, c_t, \omega_t\}$. Observed noisily
as $\theta_t$.
\end{itemize}

Context inference can work through context-specific (possibly unreliable) cues,
or through the dynamics themselves, using prediction error as the force behind
inference. Let us assume that both parts are present in the observation
$\theta_t$. In this case, the inferred generalized state is given by
\[
  q(z_t | \theta_t, \theta_{t-1}, a_{t-1}) \propto p(\theta_t | z_t)p(z_t|\theta_{t-1}, a_{t-1})
\]
  
Additionally, after having inferred the current state, motor adaptation is given by
\[
q(\gamma | z_t, z_{t-1}, a_{t-1}) \propto p(z_t | z_{t-1}, a_{t-1}, \gamma)p(z_{t-1})p(\gamma)
\]

where $\gamma$ are the parameters of the internal model for the dynamics,
i.e. the function $f: (z_t, a_t) \rightarrow z_{t+1}$, which determines the beliefs of
the agent as to how the system evolves after having taken action $a_t$.

Having inferred the generalized state (including context), a decision is made
sampling from $p(a_t | z_t, \beta)$, which is a distribution that is assumed to
be parametrized by $\beta$. These parameters can be updated via:
\[
q(\beta | z_t, \theta_{t+1}, a_t) \propto p(\theta_{t+1} | \beta, z_t,
a_t)p(\beta) \label{eqn:update-parameters}
\]

\subsubsection{Expected results}
I predict that this model will display all the experimental findings that other
models do, without invoking a possibly-infinite hierarchy of time scales, each
with their own learning rate. Here I briefly go through the experimental
findings and why I expect the model to reproduce them.

Savings. This refers to the effect that, while learning a new context, if
learning is interrupted by going back to a baseline context (e.g. no force
field) and the new context is later retaken, the new dynamics do not need to be
learned from scratch. Instead, learning continues where it was left off, with
the possibility of some performance having been lost (i.e. learning having been
forgotten). The Bayesian lerner displays this behavior simply by virtue of the
learning of contex-specific action rules (learned dynamics), which are ``stored
away'' once the context is no longer the new one (i.e. it has returned to
baseline).

Retrograde interference. This refers to the learning of context A interfering
with the learning of context B. This occurs, for example, if context A has a
force field that pushes right, while context B has a force field that pushes
left. In this case, learning B is made slower by having learned Abefore,
compared to learning B without having encountered A. This effect will be
observed in the behavior of the model if context inference needs to accumulate
before the agent is sure that a context switch has occured. In the meantime,
context A's learned dynamics will interfere with context B's because actions are
taken with a rule that depends on context (via $z_t$). Related to this, the
model makes three predictions: (1) Contextual cues (that can be trusted) will
reduce interference. (2) The amount of learning on contexts A and B will affect
intereference such that highly trained contexts will interfere more
strongly. (3) The duration of interference will be reduced in the case of
highly-learned, opposing contexts, as the prediction error of the outcomes of
actions will quickly drive contexts to switch to the current one.

Rapid de-adaptation. This is the phenomenon in which forgetting an adaptation is
faster than learning it. This would happen simply through context-based
learning, where getting out of a context means that adaptations are no longer
used (and eventually are forgotten). Note: I need to read more on this
phenomenon, as I might be misinterpreting what it means.

Spontaneous recovery. In an A-B-error\_clamp paradigm, in which during B the
opposite adaptation to A is learned, at the beginning of the error-clamp phase
(after B) participants seem to quickly gravitate towards A (thinking that A is
now active?). This would be displayed by the model via the state inference, as
now the visual cue and the observed dynamics point to different contexts. In
time, updating $p(context | visual cue)$ will lead to spontaneous recovery.

Bicycle effect. We never forget how to ride a bicycle. Existing models would
need to incorporate an almost-infinite hierarchy of time scales to account for
this, but my Bayesian learner naturally accounts for it by making use of
probability distributions: the longer you practice something, the more
entrenched the adaptation becomes by virtue of the posterior probability
becoming very peaky.


\end{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{chapter}{Behavioral findings, Physiology and neurons}
A few notes from \cite{Wolpert_Are_1995}:
\begin{enumerate}
\item Hand paths become straight with practice (Gurevich 93, Shadmehr and
Musa-Ivaldi 94)
\item Deafferented patients move differently (Ghez 90)
\item Bell-shaped speed emerges from dynamics of control, not from planning
(Jordan 94)
\item Curvature matches the participant's messed up perception of straight lines
(wolpert 94)

\end{enumerate}

\section{Notes from \cite{Harris_Signaldependent_1998}}
\begin{enumerate}
\item Arm stiffness can be adapted (Burdet et al 01.
\end{enumerate}

\section{Notes from \cite{Bays_Computational_2007}}
\begin{enumerate}
\item We have the ability to recall previously learned dynamics even after
months! (Bashers-Krug et al 96, Gandolfo et al 96).
\end{enumerate}

\section{Notes from \cite{Shadmehr_Adaptive_1994}}
\begin{enumerate}
\item When planning movement, the target is transformed from a retinocentric
vector to a head-centered and finally a shoulder-centered one. (Andersen\_85,
Soechting and Flanders\_91).
\item Gordon\_93 says that the target is finally represented as a vector from
the current hand positoin (or whatever the effector is) to the goal position.
\item From Lackner\_and\_Dizio\_92, aftereffects exist when introduced Coriolis
forces are withdrawn. They also exist when perception is mucked with with prism
glasses (Held\_59, 62, 63).
\end{enumerate}


\section{Notes from \cite{deC.Hamilton_scaling_2004}}
\begin{enumerate}
\item ``Muscle activity in specific tasks shows stereotypy whether subjects
generate force with the fingertips (Valero-Cuevas et al. 1998), the wrist
(Hoffman and Strick 1999), the neck (Vasavada et al. 2002) or the arm (van
Zuylen et al. 1988; Flanders and Soechting 1990; Buchanan et al. 1993; van
Bolhuis and Gielen 1997).''
\item ``Primates also show repeatable patterns of muscle activation when
grasping the same object repeatedly, and different activation patterns for
different objects (Brochier et al. 2001). This suggests that stereotypy is a
general characteristic of movement and is not unique to humans.''
\item There is a paragraph that discusses different cost functions. For example,
one is based on the total torque, another on fatigue and endurance, and other
mathematical alchemy. Lots of references there too.

\end{enumerate}

\section{Notes from \cite{Kojima_Memory_2004}}
\begin{enumerate}
\item ``Previous studies have shown that adaptation is relatively specific to
the size and direction of target eccentricity (Deubel et al., 1987; Semmlow et
al., 1989; Frens and van Opstal, 1994; Straube et al., 1997; Noto et al., 1999;
Watanabe et al., 2000).''
\item ``It has been demonstrated that adaptation progresses more slowly when the
subject is exposed to adapting target steps of a variety of sizes and directions
in humans (Miller et al., 1981) and monkeys (Scudder et al., 1998).''
\end{enumerate}


\section{Notes from \cite{Lee_Dual_2009}}
``When given contextual cues and sufficient trials, humans can simultaneously
adapt to two opposite force fields (Osu et al., 2004; Nozaki et al., 2006;
Howard et al., 2008), two saccadic gains (Shelhamer et al., 2005), or several
visuomotor rotations (Imamizu et al., 2007; Choi et al., 2008).''


\section{Notes from }
There is extensive evidence that fast trial-by-trial error-based learning relies on the cerebellum. Patients with cerebellar lesions show substantial impairment in fast adaptation across many task domains51–55

\section{Motor adaptation}
In typical motor adaptation experiments \citep[e.g.][]{Medina_Mechanism_2001,
  Davidson_Scaling_2004,Kojima_Memory_2004}, the goal of the participant is to
move some squishy appendage (e.g. eyes, hand) from point A to point B,
repeatedly, under different circumstances. These experiments consist of a
number of phases in different order (and some of them not present in some
experiments). These phases are the following:
\begin{enumerate}
\item Baseline (O): Participants move their appendage from A to B without being
hindered, as they normally would in their life.
\item Adaptation (A, B, ...): In this phase, the circumstances in which the movement
happens (still from A to B) are changed. For example, in human experiments with
arm movement, the arm will be subjected to forces by an external actor (e.g. a
mechanical arm attached to the participant's hand). The participant's goal is
to learn to counteract these forces and perform the movement from A to B as
well as in Baseline.
\item De-adaptation: In this phase, the circumstances are changed, either
reverted to Baseline or to some other circumstance. For example, the forces
applied by the mechanical arm are removed, or their direction is changed to the
opposite.
\item Error-clamp (E): In this phase, the error in the movement is forced to
zero. For example, the mechanical arm applies a force that forces the
participant's hand to move in a straight line from A to B, even if the
participant applies perpendicular forces.
\end{enumerate}

Using these phases, several phenomena has been shown:
\begin{enumerate}
\item Savings: Learning a previously-learned environment (after some extinction
phase) is faster than the original learning. This means that the previously
learned model is saved for a while and its learning can be resumed after
extinction. However, \cite{Kojima_Memory_2004} showed that savings can be
``washed out'' if baseline trials (no force field) are inserted between
extinction and the second learning phase; their findings are perhaps confounded
by the passage of time, as this inserted baseline makes a lot of time pass.
\item Anterograde interference: Learning an adaptation (e.g. force field
pointing left) slows down learning an opposing adaptation (e.g. force field
pointing right).
\item Rapid de-adaptation: Unlearning an adaptation (even after fully learning
it) is faster than learning it \citep{Davidson_Scaling_2004}
\item Spontaneous recovery: After a de-adaptation phase, an error-clamp phase
is introduced and participants show a rapid adaptation in the direction of A,
which slowly goes back to zero across hundreds (?) of trials.
\end{enumerate}

The experiment carried out with monkeys by \cite{Kojima_Memory_2004} used
different structures (see their figure 1C). For example, to show savings they
used O-A-B-A, where in A and B the saccade target moved during the saccade to
make the monkey think that their saccades were off. During A and B, opposite
directions were used (e.g. in A the monkeys had to learn to increase their
gain, and in B learn to decrease it). They found savings in the form of a quick
learning period during the second A block that took the monkey close to the
adaptation at the end of the first A block (see their figure 3A); after that,
learning proceeded as before.

Using an O-A-B-O-A paradigm, they showed that savings were no longer
present. On the other hand, using an O-A-B-E-A paradigm (where the error-clamp
phase consisted on putting the monkey in the dark) they found savings (their
figures 5A and 7A).

\cite{Smith_Interacting_2006} showed spontaneous recovery in a O-A-B-E
paradigm. From their data it is unclear what exactly happens during B, but
during E it seems like participants adapted a bit in the direction of B and
then quickly jumped to the direction of A, from which they slowly went to zero.

Similarly, \cite{Ethier_Spontaneous_2008} who spontaneous recovery with
saccades in an E-A-B-E paradigm. From their results (e.g. their figure 2) it
seems possible that some participants go back to A's adaptation and some others
to O's. Additionally, they introduced 30s breaks during adaptation and showed
that these breaks had a ``forgetting'' effect, i.e. adaptation was set back by
a small amount after each break. The further posit that introducing such a
break before the second E phase would eliminate spontaneous recovery entirely.

\cite{Huang_Persistence_2009} showed that if participants are placed in an
adaptation environment where the force gradually ramps up (across many trials),
participants show slower de-adaptation during an error-clamp phase, compared to
participants in which the force was suddenly increased during adaptation. They
also showed that participants alter their learning rates to better fit the
volatility of the environment, regardless of the direction of adaptation.




\section{Notes on savings, quick deadaptation and contextual cues}
In \citep{Kojima_Memory_2004}, Figure 1a-b, we can see that de-adaptation was a
bit faster than adaptation for the gain-increase experiment, and much faster
for the gain-decrease experiment. It seems like there is no contextual cue
available to the monkey, as all trials (over 1k) were presented one after the
other with no breaks. Poor monkey.

\citep{Kojima_Memory_2004} additionally show savings in the form of
re-adaptation that, for a few hundred trials, is much faster than the original
adaptation (control adaptation). This can be better seen in Figure3a-b. They
discuss that there is even a point of inflection in test-adaptation that
clearly marks the end of savings and resuming of adaptation. The duration of
the savings phase changed from experiment to experiment, from monkey to
monkey. The authors do not check whether the past-facilitation rate of learning
is similar to that of the end of the control adaptation phase, which would
indicate that the monkey quickly went back to where it had left off.

Finally, they show that savings seem to disappear after a period of
error-clamp-like trials, in which the ISS is set to zero, thus allowing the
monkey hundreds of trials in which his adapted saccades (gain != 1) yielded
correct results. In contrast, savings do not disappear after the monkey head
was covered with a black cloth, placing him in total darkness (no feedback for
his saccades).

Given the lack of contextual cues, context identification could be very slow in
monkeys, which would explain the facilitation period. During normometric
saccades after adaptation (error-clamp-like trials), context inference could
have been cimented to the point that it became difficult for the monkey to
shift away from it once test adaptation began, thus removing savings from the
equation. During darkness trials, in contrast, there is no contextual evidence
in any direction, and thus the precision over context does not change. In this
case, savings are back on the table.

\cite{Ethier_Spontaneous_2008} present a simple experiments with saccade
adaptation in humans. In their paradigm, humans go through a O-A-B-E paradigm,
in which the adaptation is forced by making the target move closer or further
away than it was before, making participants believe that they had
over/under-shot the target. While this study does not quite talk about savings,
there is evidence of context-based savings in their paradigm, as adaptation
trials were done in blocks of 60 trials, with 30s pauses between blocks, in
which participants closed their eyes. In their figure 2, they show that each
pause caused a dip in the observed adaptation. While this pause does not
provide any clues as to the identity of the following block, it does indicate
that a new block will start, which could make participants revert to using
their priors over contexts to make the first decision in the following block
and consequently using the feedback to perform context inference, explaining
the quick dip after each pause.

Their study does show quick de-adaptation during a second block of trials in
which the desired gain is opposite of the adaptation trials. However, it is
difficult to interpret these savings in terms of the context-based learner, as
learning in the original adaptation is also quite fast, and the B context had
not been introduced in advance. However, the fact that group 2 (which has
longer pauses) shows faster de-adaptation supports the idea of context-based
learning, as the longer pause before de-adaptation causes context inference to
rely more on priors and thus make the jump from model A to model O almost
immediately, and then summoning model B and learning at the same speed as model
A was learned.

Importantly, the effect of these pauses speak for continuity-based priors,
which are based on previous trials and not on prior beliefs regarding which
contexts would, in general, be encountered more often in the world.

\cite{Huang_Persistence_2009} presents interesting data, but I can't interpret
it for now.

\cite{Lee_Dual_2009} present a study that is very useful to us. In it,
participants have to use a joystick to move a cursor from its initial position
to a target, under different visuomotor rotations (tasks A, B, C and D). Of
special significance is the fact that participants were provided with very
clear contextual cues, as the target area was in a position that uniquely
identified the rotation. The effects of these cues can be seen in their figure
6, in which they show how participants very quickly learn the adaptations
during an A-B-A phase, with very quick jump from A to B and from B to A. Then,
participants enter a phase in which tasks C and D are randomly chosen every
trial. Because of the unambiguity of the contextual cues, even during learning
participants display very clear switching of internal models. This is so far
the most clear example of contextual cues causing immediate ``deadaptation''
(which we know is really switching, but don't tell anybody).

\cite{Forano_Timescales_2020} present a study with humans and a mechanical arm,
in which participants have to perform forward reaching movements under
different forces. During their experimental sessions, their complications for
which are endless and unnecessary, there is a baseline phase, in which
participants perform no-force-field movements in two different workspaces
(strong contextual cues). Then an adaptation phase starts, in which
participants do eight trials in context A, eight in B (with random Es thrown in
for good measure). Awful things happen later on, of which we do not
speak. During de-adaptation, they follow the same 8-8 structure, but the cues
are flipped; this phase ends when the adaptation is taken back to zero. Finally
the error-clamp trials start, with a contextual cue that is the same as the
first one they got during the adaptation phase. They do not speak of savings,
and savings cannot be seen in their plot, but quick de-adaptation is clearly
seen in their figures 6 and 7. However, it is not as quick as it could be, and
I believe this to be due to the strength of the cues, which cause a lot of
conflict in the participant as the force-feedback and contextual cues are so at
odds with each other. I would argue that the slow decay (still fast compared to
adaptation) is evidence of yet another phenomenon: the devaluation (or
re-evaluation) of contextual cues, in which the precision with which they are
treated decays over time as a consequence of the apparently false information
they provide. I expect this to interact with the following error-clamp trials,
in which \cite{Forano_Timescales_2020} show small-but-significant spontaneous
recovery. I believe that if those de-adaptation trials were removed, the
increased strength of the contextual cues would make the spontaneous recovery
larger.

\cite{Davidson_Scaling_2004} tested quick de-adaptation more thoroughly than is
usual, with the explicit goal of checking whether there was something special
about A-O de-adaptation that made it quick, or if A-B de-adaptation would also
be quick. Additionally, they explored the question of which one is faster. They
used the typical curling-force mechanical arm for their experiment with human
participants. Their results can be groupped into two: (1) De-adaptation from A
to O is slower than de-adaptation from A to 1/3A. (2) De-adaptation from A to
3A is perhaps a bit faster than from A to -A, but, more importantly,
re-adaptation to A is faster coming from 3A than from -A. The first result can
be explained simply: de-adapting from A to A/3 implies less ``travel''
(i.e. adaptation) than from A to 0, and thus motor error takes longer to go
down when de-adapting to O. This is precisely why \cite{Davidson_Scaling_2004}
performed their second experiment from which (2) was obtained. Their second
result can be explained by switching internal models, if action selection is
done via a weighted sum of the actions generated by each possible model (where
the weights are the posterior probabilities of the models). In effect, the
assymetry is caused by the existance of O. In this account, going from 3A to A
more quickly reduces motor error because the contributions of 3A and O to motor
output ``cancel out'', as they point in opposite directions relative to A,
leaving only the contributions of A. When coming from -A, the contributions of
-A and O point in the same (incorrect) direction (relative to the real
generating process A), thus making motor error be bigger.

Re-adaptation in \cite{Davidson_Scaling_2004} differs greatly between -A and
2A. While the explanation above could be enough, I believe that an extra factor
would be the fact that while adapting to forces in different directions,
different muscles must be used. The transfer from using a model that uses a set
of muscles to another that uses another set of muscles might come with its own
perils. More research into this could be useful.

Interestingly, \cite{Davidson_Scaling_2004} calculated that re-adaptation from
3A to A was not significantly different in experiments 1 and 2, even though
participants in experiment 2 had already learned the A environment, while those
in experiment 1 had not. This can perhaps be explained as a fluke in the
switching mechanism, where the weight of the A model in experiment 2 made its
contribution to action lie square in the middle of those from 3A and O. This
might need further consideration.

\cite{Zarahn_Explaining_2008} performed a typical experiment with visuomotor
modifications (screen, hand position, axis rotation) to show that in their
O-A-W-A trial, where W are washout trials, savings still existed, even after
the fast-slow learner predicted that savings should have disappeared. They
showed the same with O-A-(-A)-A and who knows what else. Their figure 3 is a
nice poster child. Note that while they did not have any contextual cues, their
transformations are HUGE. In their O-A-W-A experiment, A = 45°. A = 30° in their
other experiment. This in stark contrast with many other studies that use
10°-20° ranges.

\cite{Kim_Neural_2015} present savings with very informative and clear
cues. Their savings are beautifully portraited in their figure
2A. Additionally, they present imaging results based on regressors obtained
from the fast-slow linear models.

\section{Notes on the effects of context inference on learning}

\cite{Herzfeld_memory_2014} present what is possibly (so far) the most
compelling evidence for the effects of contex-inference on learning itself,
uncoupled from motor responses. The authors make the point (with a model and
experiment) that the brain adjusts its sensitivity to errors based on the
volatility of the environment. Moreover, this sensitivity depends on the size
of the error, such that a participants' learning might be faster when facing
small errors than with large ones. In their first experiment, the authors
divided participants into three groups that differed only on the volatility of
the environment, group 1 facing the most stable environment, where the
probability that the perturbation changed from 13 to -13 (N s/m) was 0.1; group
2 had a probability of 0.5 and group3 of 0.9. \cite{Herzfeld_memory_2014}
labeled these groups as 0.9, 0.5 and 0.1, respectively. Here I keep with these
labels.

\cite{Herzfeld_memory_2014} showed in their figure 1D that the stable
environment of group 0.9 made participants learn more quickly from errors, with
the effects being lower for groups 0.5 and 0.1, in that order. In my model, the
interpretation of these results is fairly straightforward: more volatile
environments lead to less precise (and more often incorrect) context inference,
which would slow down learning. In particular, in the mathematical
implementations of the model so far (see \ref{sec:model-instance}), the
precision with which observations are taken in depends on the precision of the
context inference, which should create behavior like that observed by
\cite{Herzfeld_memory_2014}. Note that the authors' results in experiment 2 are
explained by my model in very much the same vein.

One puzzling result from \citep{Herzfeld_memory_2014} is that sensitivity to
error depends on the size of the error. With some effort (invested in
understanding their paper...), this can be very nicely explained by the
context-inference model. First, to put some of their numbers in perspective,
their second experiment implies errors roughly in the range of -2 to 2 (which
matches the x-axis in Figure 2). An error the size of -2 or 2 very clearly
signals the change from one context to the other (0.9 gain to 1.1, or vice
versa; it is unclear in the paper). Because the direction of the change is
clear evidence from the previous and current contexts, context inference right
after such a switch becomes very clear (the maximum of the posterior $>>$ the
minimum of the posterior), which makes learning somewhat fast and does not
depend on the volatility of the environment, at least in the first trial after
a switch; during the next trials, the uncertainty increases again due to
volatility. The smaller the error is (in absolute value), the less clearly it
can be attributed to a switch in the environment, to the extreme where changes
are minuscule and thus must be attributed to the motor adaptation's error; it
is because of this that small errors have a higher effect on learning. Now, for
smaller errors, there is a clear effect of volatility on learning. I believe
this can be explained by the context-learner: Because of the overall slower
learning in volatile environments, the internal models are, for any given
trial, not as well learned for the high-volatility groups, which leads to many
errors being misinterpreted as context switches (for which no learning might
occur), making the average look worse than it is (except, as explained above,
when the error is close to 2 or -2, which makes switches very evident).

In fact, this introduces important confounds: errors of exactly 2 and -2 can
only ocurr when the participant has learned both internal models perfectly and
simply predicted the wrong one, thus adapting in the incorrect direction. In
this case, learning should be exactly 0. However, errors very close to, but not
exactly, 2 and -2 can be attributed to bad context inference plus the need to
adapt the ``other'' model, i.e. the model that was not predicted. In this case,
depending on how smart the participant is, the other model might be updated
with an error sensitivity as high as that of the near zero error. On the other
hand, participants observing an error close to zero might interpret this as a
signal to update an internal model (in which case the sensitivity should be
high), but they might also falsely (or even correctly) accuse the context of
having changed, but with low precision, making learning of the ``other'' model
very low (due to low precision on context), or even zero, if the participant is
so unsure that she does not want to learn from this. Because of this, a more
correct version of Figure 2E would take into account how well the internal
models have been learned at each trial, how many trials were spent in
perfect-model land, how many times the participant took a near-2 error as a
switch (correctly and incorrectly), how many times near-2 errors occurred when
the models been already fully learned, etc.

The third experiment by \cite{Herzfeld_memory_2014} has too many
confounds. They conclude that they can independently manipulate sensitivity to
big and small errors, but I believe their experimental design has unwanted
consequences because 8N and -8N were shown a lot during the fast-switch part,
but 4N and -4N were not. It's such a mess that I don't want to touch it.



\section{Notes on error-clamp weirdness}
\cite{Kojima_Memory_2004} have error-clamp-like trials, but not at the end. No
weirdness here. Also no cues.

\cite{Smith_Interacting_2006} present a typical mechanical arm task with a
O-A-(-A)-E structure, with random E trials thrown in throughout all. Half of
the trials were with outward movements and half with the inward movement, with
the latter almost being E trials and not used for analysis. The experiment has
no cues, but it was a design in which blocks of 60 trials were used, presumably
with pauses in the middle, though they do not explicitly say this. The authors
find that during the E block at the end, participants' responses show
spontaneous recovery, where they seem to be moving towards A for a few trials,
but never quite reaching it, and then taper off to
zero. \cite{Smith_Interacting_2006} only show data aggregated over all
participants, so it's difficult to extract too much information, but their
figure 3D suggests that some participants assume -A during E, while most assume
A.

\cite{Ethier_Spontaneous_2008} present a saccade study (movements of 15° with
adaptation of +-5°) in which participants go through E-A-(-A)-E blocks, each
divided into sets of 60 trials (some blocks have different sets). Between each
set (and block), a pause of 30s happened. There are no cues, although the size
of the adaptations is big. It is important to note that the way the error clamp
works in this paradigm probably gives no indication to the participant that an
error has occured; contrast this with mechanical-arm paradigms, or
visuorotation reaching paradigms, in which proprioceptive information gives
feedback to the participant regardless of rewards.

Figure 2 in \cite{Ethier_Spontaneous_2008} contains all the results relevant to
this section. First of all, it is clear from the first E block that spontaneous
recovery and slow tapering off do not exist if no adaptation has been learned;
instead, behavior during these trials is the same as during baseline. Second,
spontaneous recovery happens always in the direction of the first adaptation A,
and never in the direction of -A, which is the block right before the last E
block. The context-inference-based model explains these findings by noting that
the -A block is very short, ending when participants reach 0 adaptation (during
``de-adaptation'') or a little bit beyond zero (towards -A). This is important
because, since adaptation was not finished, during the first trial of the E
block the participant expected an error, but no error was observed. This leads
the participant to think that a change in the context has occured, and context
inference kicks in, taking the lack of an error as evidence in favor of either
baseline or the previous adaptation A. Given that the action by the participant
on this trial is more consistent with baseline than with A, why would A play
any role in future responses? The answer lies in the structure of the
experiment, as no baseline trials were ever performed in front of the screen,
which makes baseline have very low priors.

\cite{Forano_Timescales_2020} has spontaneous recovery and slow tapering off
during error-clamp trials, but it's quite small. Maybe because of the large
de-adaptation that devalues contextual cues?

\cite{Vaswani_Decay_2013} explores in detail the motor decay observed during EC
trials after adaptation trials in a viscous field paradigm. They conclude,
among other things, that the motor adaptation decays across as many as hundreds
of trials, but never goes back to zero. Additionally, there is a lag before
decay starts (all their experiments end in A-E, so no baseline before), and it
depends on the participant, as well as how noticeable the transition to
error-clamp trials is. They have some clever experiments.

More precisely, \cite{Vaswani_Decay_2013} have an experiment 1 in which
participants learn an adaptation and then go through hundreds of trials of
error clamp. Participants are divided into four groups: groups 1.1 and 1.4
learn adaptation A and -A, respectively, and then go to EC trials. Group 1.2
performs the task in baseline and then in A. Group 1.3 performs the task in
-A/2, then A and then EC. In their results, all groups show ``adaptation'' that
starts high (consistent with A) and, after some lag, goes down to an adaptation
of almost zero, but decidedly positive. Group 1.3, in particular, decays faster
than the other groups but does not seem to get closer to zero even at the end
of the experiment. Group 1.4 is just like 1.1 and 1.2, but with its sign
flipped.

Our simulations can easily show these results to be a direct consequence of
context inference, which starts by assuming that no change has happened after
EC starts (thus staying with A), but eventually the evidence points towards a
change. However, because no model explains EC fully, actions are taken by doing
a weighted average of all available models, where the weights are given by the
posterior over contexts. This explains both the lag, the fact that EC
adaptation never reaches zero and that group 1.3 adapted faster, as the -A/2
model pulls the average down faster than baseline. The fact that no difference
is observed between 1.1 and 1.2 is due to the fact that the baseline model
exists, regardless of whether participants trained on it or not. Just like the
results from \cite{Davidson_Scaling_2004}, these results point toward motor
output being a weighted average of all the available models, with weights being
the posterior over contexts.

The conclusions of experiments 2 and 3 by \cite{Vaswani_Decay_2013} reinforce
the context-inference-based account: the more difficult it is to notice that EC
has started, the less decay participants show, to the extreme of experiment 3
in which most participants showed no decay at all.

\cite{Pekny_Protection_2011} present a series of experiments in which they set
out to determine whether A-B-E paradigms make the brain forget about A when
learning B, or whether the brain protects A. The authors mention that there is
a lot of literature that has used savings as a way to measure the protection of
A and found that A is not being protected; however, the authors argue that
classical conditioning experiments instead use spontaneous recovery during E
trials to measure protection of memories during extinction, and the literature
in that field has demonstrated that memories are indeed
protected. \cite{Pekny_Protection_2011} present a series of experiments with
mechanical arms to show the protection of memories. Their results, specifically
those in their Figures 3 and 5, strongly support not only the
context-inference-based motor learner, but also the idea that action selection
happens through a weighted average, much like the results from
\cite{Herzfeld_memory_2014}.

In figure 3A, one group (red) went through O-A-b (where lowercase ``b'' means
very few trials, used as extinction), and another group (black) went through
O-B-A-b. When comparing the force compensation between those two groups, one
can see that the introduction of B in the black group has the effect of pulling
their response during E closer to baseline, as compared with the red
group. This is because B has, for this group, been introduced as a potential
context of importance equal that of A (although a longer time ago, as B is
presented before A). This increased the prior probability of context B during
the E trials in the minds of participants, which draws the weighted average
closer to baseline, as $B = -A$. The same interpretation can be given to all of
Figure 3 and Figure 5 as well.

Additionally, \cite{Pekny_Protection_2011} present an experiment in which
changes in context are presented gradually (see their figure 1 for
details). They argue that because the changes are presented gradually,
participants do not get a strong prediction error that can guide context
inference towards the correct context. However, there is a caveat they do not
discuss: in an experiment where context switches slowly, small changes become
big evidence. For this reason, their Figure 5B has the same effect as that seen
in Figure 3.


\section{Notes on implicit vs explicit, and strategies.}
The idea behind strategies in motor adaptation is simple: participants can be
told how to perform in a novel environment. Then, when the novel environment
kicks in, there is no period of adaptation before performance is near-perfect.

For example, \cite{Taylor_Flexible_2011,Mazzoni_Implicit_2006} showed that in a
visuomotor rotation task, participants can be given the hint to ``do as if you
were aiming for this other target'', which allows them to complensate almost
perfectly from trial 1. However, the authors also found interesting effects:
while at first participants have zero error, they slowly drift away from this,
overcompensating for the visuomotor rotation. The authors showed that this
overcompensation goes away, given sufficient trials.

\cite{Taylor_Flexible_2011} present a computational model based on the linear,
prediction-error based models \citep[e.g.][]{Smith_Interacting_2006} by making
prediction error incorporate some element of the strategy. This model is fine
and explains the observed phenomena, but I believe that they overlooked the
mismatch between propriosceptive and visual feedback. This mismatch, which is
independent of reward, would explain their results (at least qualitatively). I
would speculate that the effect of the strategy given to the participants by
\cite{Taylor_Flexible_2011} makes them not adapt their motor rotations, but
rather shift the goal of the trial. Then, as trials go by, the propiosceptive
feedback does not match what is observed on the screen, and that causes motor
adaptation. Perhaps more recent studies have tested this.


\end{chapter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{chapter}{Notes on papers}
Conclusions and useful things about different papers I read. This differs from
the comments above in that the notes here are holistic.

\section{\cite{Shadmehr_Adaptive_1994}}
In this paper they show a number of things. First, the show that people can
adapt the dynamics of their control states to match changing dynamics of the
observers. They also show that this adaptation takes the form of a straightening
of trajectories, which are very twisted when they are first introduced to new
system dynamics (viscosity of the environment); participants learn to do
straight lines in the new environment after some trials, and the learing is
monotonically increasing. They show a model of the dynamics of the arm, the
control states and the adaptation that happens when exposed to the new
dynamics. Finally, they show, both experimentally and in their model, the
aftereffects of the new dynamics, whereby they display bad behavior when they go
back to the natural environment, but eventually adapt to it as well.

Interesting paper that cound be the foundation of our models, assuming that it
hasn't been improved upon and maybe entirely replaced by newer stuff.

A description of their model can be seen in \ref{subsec:shadmehr-1994} above.

\section{\cite{Todorov_Optimal_2002}}
See \ref{subsec:todorov-2002}

\section{\cite{deC.Hamilton_scaling_2004}}
The authors make use of the TOPS strategy \citep{Harris_Signaldependent_1998}
and an experiment where they measure torque at different joints to show that
bigger muscles produce less variability than smaller muscles for the same level
of voluntary torque (I wonder what the opposite of voluntary is).

It's an experimental paper with some muscle simulations thrown in for good
measure. Contains a lot of references that might be useful later on the nature
and characteristics of human and primate movements, as well as cost functions
used in the past for modeling them.

\section{\cite{Beers_When_2002}}
This paper speaks of a model of optimal adaptation and weighting of visual and
proprioceptive cues. Nothing of interest for us as of now.

\section{\cite{Scheidt_Learning_2001}}
Humans can adapt their movements to stochasticly varying changes in the dynamics
of the environment. Moreover, they showed that their adaptation is most
complatible with the idea that participants adapt to the mean of the dynamics (a
one-directional force of changing magnitude) of the environment instead of doing
trial-to-trial adaptations.

These views, however, are challenged by the two-time-scales models of learning,
like \cite{Smith_Interacting_2006}.

\section{\cite{DeKleijn_Everyday_2014}}
They cite \cite{Henry_Increased_1960} saying that participants have longer
reaction times when preparing complex movements than when preparing simple
ones. May be worth checking out.

\section{\cite{Haar_Motor_2020}}
This is the only paper I know where they do motor learning in a very natural
task: playing billiards. It's great and I wish I could do this research.

They tracked the joints of participants who were doing the same pool shot over
and over (300 times). I do not think there was any variation in initial
positions. They track performance and variability in joint movement.

\section{\cite{Wolpert_Principles_2011}}
Good review on motor learning. many relevant citations here.

\section{\cite{Todorov_Optimality_2004}}
Review of optimal control. Citation goldmine! Lots of papers on optimal control
in biological systems are cited here.

In particular, [43] and [44] might be similar to the idea of inferring goals and
predicting future movements.

``However, feedback control[25] explains an important additional observation:
the increased duration of more accurate movements is due to a prolonged
deceleration phase, making the speed profiles significantly skewed[88].''

\end{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{chapter}{Implementing the adaptation model} In this section, I propose a
simulated task in which the prowess of the model proposed in
\ref{subsec:proposal} is put to the test. I additionally present simulation
results and some writeup. Basically a mini-paper on this model.

\section{The Task} The tasks used for models like this one are usually
\citep[e.g.][]{Lee_Dual_2009} reaching tasks: a participant sits in front of a
table with shoulders immobilized, and has to bring her hand from the starting
position A to a designated position B. This is done while holding a robotic arm
that moves with the hand. The robotic arm is used to create force fields in
different directions (CW and CCW, typically, but it's basically left and right)
for the different contexts. The position of the participant's arm is recorded
and deviations from the straight line are considered as errors ($e(t)$ in
\ref{eqn:lee-learn}).

To simplify the task, I propose the following: participants need to keep their
hand still in the starting position for the duration of the trial. The robotic
arm creates forces that will drive the arm away from its current position by
pushing in a constant direction with constant force. This experiment has the
same elements as the one by \cite{Lee_Dual_2009} but eliminates the necessity
of an underlying movement. While it works wonders as a thought experiment and
helps me see if the proposed model behaves the way I want, it must not be used
as a real experiment because participants could simply use impedance for each
trial, regardless of context.

In this experiment, every context $C_i$ is characterized by the strength and
8'rection of its force: $C_i = \{\theta_i, \gamma_i\}$, where $i = 1, 2, ...,
N_c$, $N_c$ is the number of contexts, $\theta$ is the angle of the force
measured Cartesianly, and $w_i$ is the intensity of the force.

As in \cite{Lee_Dual_2009}, I will simulate an A-B-clamp paradigm, which starts
with a context $C_0 = \{0, 0\}$ in which no force is being applied. Then
context $C_A$ is shown for a few trials, then context $C_B$, then an
error-clamp block in which the hand is forced to stay at the initial position
by the robotic arm.

Additionally, a contextual cue $\omega$ is shown to the participant which
signals the current context. The experiment will be run with different levels
of reliability of this cue. Different sessions will be run with different cue
reliabilities.

An experimental session $\Xi$ is characterized by the reliability of the cue,
the contexts avaiable and the order in which they are shown. In a given
session, each trial begins with the cue and the XXX seconds during which the
participant must keep the hand in the starting position. During the intertrial
time, no forces are applied to the hand, but the hand must remain in the
starting position.
 
\section{The model}
\label{sec:model-instance}
The model is an implementation of \ref{subsec:proposal}. The generalized state
is given by:
\begin{equation}
z_t = \{x, \theta_t, \gamma_t, \omega_t\}
\end{equation}
where $x$ is the current position of the hand in Cartesian coordinates,
where the origin is the starting point. We assume that motor commands are
issued every $\Delta t$ for simplicity. At the beginning of each time interval,
the context is inferred combining the cue and the prediction error of the
outcome of the previous motor command. We assume further than the cue and the
force field are uncorrelated, which yields:
\begin{align}
  q(C_t) &= q(C_t | \omega_t)q(C_t | s_t, s_{t-1}, a_{t-1}) \\ \label{eqn:estimated-context}
  q(C_t | \omega_t) &\propto p(\omega_t | C_t)p(C_t) \\
  q(C_t | x_t, x_{t-1}, a_{t-1}) &\propto p(x_t | C_t, x_{t-1}, a_{t-1})p(x_t)p(x_{t-1})p(C_t)
\end{align}
The terms $p(C_t)$ refer to the prior probability of the context at
the beginning of the trial: at the first trial, this refers to prior beliefs
over which contexts are more common/likely. At each subsequent trial, it
incorporates the belief that has so far been accumulated, given previous
trials.

Once the posterior over contexts has been calculated for the current trial, it
is used to select the current context. A simple rule is to sample the context
directly from the posterior distribution. However, a useful extension of this
is to use a softmax function to create a distribution:
 !''¿
\begin{equation}
S(C^{(t)}) = \frac{e^{\kappa
q\left(C^{(t)}\right)}}{\displaystyle \sum_{i=1}^{N_c}e^{\kappa
q\left(C_i\right)}}
\end{equation}
where $q(C_i)$ is given by \eref{eqn:estimated-context}. The
function $S(\cdot)$ is then used to sample the (inferred) current context. This
representation has the advantage of adding a parameter with which the sampling
behavior can be characterized, ranging from choosing uniformly when $\kappa =
0$ to choosing the context with the highest probability deterministically as
$\kappa \rightarrow \infty$.

In the case where the hand position can be determined by the participant with
perfect accuracy (ignoring visual and proprioceptive error), we have that
$p(x_\tau) = \delta(x_\tau, \hat x)$, where $\hat x$ is the real position of
the hand, and $\delta (\cdot, \cdot)$ is the Kronecker delta function. We
assume this to be the case during this experiment.

  For simplicity, we assume that participants know how many distinct contexts
will be shown to them, and therefore all the $C_x$ have been created by the
agent before the session begins. By default, all contexts assume a force with
angle and magnitude equal zero, and their true values are learned during the
experiment (see below).
1234568¿|
For mathematical tractability (i.e. to avoid circular distributions), we will
limit contexts to one where the force points left (angle $\pi$) and one where
the force points right (angle 0), in addition to the baseline context. In this
scheme, participants need only learn the magnitude of the force for each
context.

The generative model for the force can take on different shapes for the prior
and posterior distributions. In this work, we explore three different forms of
this generative model which differ on the assumptions the agent makes regarding
how the force fields are being generated by the environment. Each version of
the model is discussed separately below and their pros and cons discussed;
additionally, simulations are presented in the following sections.

To infer the parameters of the posterior distributions, we make use of Bayes'
theorem. In the following equations, we drop the dependence on the context and
observations for notational simplicity:
\begin{equation}
q(\theta | x_{1:t}) \propto p(x_{1:t} | \theta)p(\beta)
\end{equation}
where $\theta$ are the parameters of the force field to be estimated and
$x_{1:t}$ are the data observed until time $t$. Across all variants of the
model presented below, the likelihood will be a Gaussian:
\begin{align}
  p(x_t | \theta) &= N(\mu_x, \sigma_x)  \\
  \mu_x &= \theta_{t-1} + x \\
  \sigma_x &= \xi_0 1
\end{align}
This likelihood is the probability of observing $x$ given the previous estimate
of the parameters of the force field $\theta_{t-1}$, and given the dynamics of
the system (see below).

The final part of the model to establish is action selection. Action selection
is done at the level of desired outcome (as opposed to motor commands)
following the equations:
\begin{align}
  x_{t+1} &= x_t + a_t + f_t + \sqrt{\Delta
            t}\epsilon\\ \label{eqn:dynamics}
  f_t &= \gamma_t (cos\theta_t, sin\theta_t) \\
  a_t &= \argmax_{a_t}p(x_{t+1} = (0, 0) | x_t, a_t, f_t)
\end{align}
where $\gamma_t$ and $\theta_t$ are the participant's estimate of
the parameters, given by \eref{eqn:estimated-context}. $\epsilon$ is a Gaussian
motor error component.



\subsection{Left-right with fixed force value (LR)}
In this version, the force field is assumed to have a fixed value which the
agent tries to infer. The agent's uncertainty on this inference is reflected on
the parameters of the posterior over the force directly. For simplicity, we
assume that the priors (and posteriors) are normal with mean and standard
deviation to be updated at every time step. Therefore, the posterior over the
magnitude of the force for each context is given by:
\begin{equation}
q_t(\gamma) = N(\mu_{\gamma}, \sigma_\gamma)
\end{equation}
where $q_t(\gamma)$ is the posterior of the magnitude $\gamma$ of the force
field distribution after the observations at time $t$, and $\mu_\gamma$ and
$\sigma_\gamma$ are the inferred parameters of the distribution.

Because the likelihood is a Gaussian with support $\mu_\gamma$, the update
equations are given by:
\begin{align}
  \mu_\gamma^{(t)} &= \frac{\mu_\gamma^{(t-1)}\left(\sigma_\gamma^{(d)}\right)^2 + \mu_\gamma^{(d)}\left(\sigma_\gamma^{(t-1)}\right)^2}{\left(\sigma_\gamma^{(t-1)}\right)^2 + \left(\sigma_\gamma^{(d)}\right)^2} \\
  \sigma_\gamma^{(t)} & = \sqrt{\frac{\left(\sigma_\gamma^{(t-1)}\right)^2\left(\sigma_\gamma^{(d)}\right)^2}{\left(\sigma_\gamma^{(t-1)}\right)^2 + \left(\sigma_\gamma^{(d)}\right)^2}}
\end{align}
where the superscripts $(t)$ and $(t-1)$ refer to the inferred values for the current and previous trials, respectively, and $(d)$ to the parameters in \eref{eqn:data-dist}.

\subsection{Left-right with unknown mean (LRS)}
In this model, the force at each trial is assumed to be sampled from a normal
distribution and the mean of this distribution is inferred from the
observations, while the standard deviation is assumed to be known. The mean of
the force is assumed to also be normal with unknown mean and standard
deviation:
\begin{align}
  p(\gamma) &= N(\mu_t, \sigma) \\
  p(\mu_t) &= N(\mu_mu, \sigma_\mu)
\end{align}
In this manner, the uncertainty for this agent is at the level of the estimate
of the parameters of the force field, as opposed to being directly on the
force. Because the priors for $\mu_\mu$ are normal and the standard deviation
$\sigma$ (not $\sigma_mu$) is known, the update equations are given by:
\begin{align}
  \mu_\mu &= \frac{\sigma^2}{\sigma^2 + \sigma_\mu^2}\mu_\mu + \frac{\sigma_\mu^2}{\sigma^2 + \sigma_\mu^2} x \\
  \sigma_\mu &= \frac{\sigma^2\sigma_\mu^2}{\sigma^2 + \sigma_\mu^2}
\end{align}
where $x$ is the observation.

\subsection{Left-right with unknown mean and standard deviation (LRMS)}
In this version, we will assume that for each context, the agent's belief over
the magnitude of the force is given by a normal distribution:
\begin{equation}
p(\gamma) = N(\mu_f, \sigma_f) \label{eqn:data-dist}
\end{equation}
where $\mu_f$ and $\sigma_f$ are parameters to be estimated at each
trial. Before the experiment begins, the agent will have a prior distribution
over these parameters. A standard Bayesian approach is to choose NormalGamma
priors:
\begin{equation}
p(\mu_f, \sigma_f) = NG(\mu_0, \nu_0, \alpha_0, \beta_0)
\end{equation}
where $\mu_0, \nu_0, \alpha_0$ and $\beta_0$ are free hyperparameters of the
model. Because participants are told that there are three contexts (baseline,
left and right), we assume the following hyperparameter values:
\begin{align} \mu_f^{(0)} &= (0, -1, 1) \\
  \nu_f^{(0)} &= 0.1 \\
  \alpha_f^{(0)} &= 1 \\
  \beta_f^{(0)} &= 0.5
\end{align}
Note that $\mu_f^{(0)}$ is different for each context and is thus
provided as a vector $(0, -1, 1)$ representing the baseline, left and right
contexts, respectively. For all other hyperparameters, all contexts have the
same value. These values ensure that the prior over the hyperparameters
$(\mu_f, \sigma_f)$ has the mode at $(\mu_f^{(0)}, 1)$. We will show that the
choice of these hyperparameters affects learning rate, but not the final
learned magnitude \todo{Check this}.

The update equations for the magnitud parameters are given by:
\begin{equation}
q(\mu_f, \sigma_f | x_t, x_{t-1}, a_{t-1}) \propto p(x_t |
x_{t-1}, a_{t-1}, C_t)p(\mu_f, \sigma_f) \label{eqn:context-from-x}
\end{equation}
where $p(x_t | x_{t-1}, a_{t-1}, C_t)$ is a Gaussian distribution centered
around $\mu_a$ with standard deviation $\sigma_a$, which is a free parameter of
the model. It is assumed that the standard deviation over outcomes ($\sigma_a$)
is related to that of the parameter $\mu_f$ as follows:
\begin{equation}
\sigma_f = \sigma_a / \nu_f
\end{equation}
which makes $\nu_f$ a parameter to estimate. Because we chose the priors to be
NormalGamma and the likelihood to be Gaussian, the posteriors are also
NormalGamma, and the parameters are updated after one time step via:
\begin{align}
  \mu_f^{(t)} &= \frac{\nu_f^{(t-1)} \mu_f^{(t-1)} + x_t}{\nu_f + 1} \\
  \nu_f^{(t)} &= \nu_f^{(t-1)} + 1 \\
  \alpha_f^{(t)} &= \alpha_f^{(t-1)} + 0.5 \\
  \beta_f^{(t)} &= \beta_f^{(t-1)} + \frac{\nu_f^{(t-1)}}{\nu_f^{(t-1)} +
                  1}\frac{\left(x - \mu_f^{(t-1)}\right)^2}{2}
\end{align}

The effect of the hyperparameters of the priors are worth a note or two, as they are complex.

Naturally, $\mu$ affects the initial estimate of the adaptation, in the same
units as the necessary adaptation. $\nu$ encodes how stable this hyperprior is:
higher values (e.g. 10000) all but guarantee that the hypermean will not move
in the face of evidence; In principle, enough evidence should still move it,
but that won't happen during an experiment. Smaller values (e.g. 1 / force\_sd,
as is the default on the code) make the hypermean (and thus the mean) follow
evidence more freely. Note that as more observations are accumulated, $\nu$
becames bigger and bigger, solidifying the value of the hypermean.

The hyperparameters $\alpha$ and $\beta$ are a bit more complex. Note that the
mean of a Gamma distribution is $\beta / (\alpha \nu)$; this mean is being used
as the standard deviation of a Gaussian by the rest of the agent (cheating),
which makes it an important measure of uncertainty. While setting the default
hyperparameters, the values used are $\alpha = 0.5 / \sigma_0$ and
$\beta = 0.5$, where $\sigma_0$ is the \textit{a priori} estimate of the
standard deviation of the force exerted by the environment (force\_sds in the
code), which controls the initial learning rate. This makes the initial
standard deviation equal $\sigma_0$, which makes it consistent with the
fixed-force model in its interface. The 0.5 values ensure that uncertainty is
large at the beginning and during the experiment is greatly reduced, but never
to a point where it's ``visually'' too small. Changing this 0.5 would make the
standard deviation change more quickly, making the model more or less precise
in its predictions, independently of the volatility of the mean of the
adaptation (via the hypermean).

The baseline model defaults to different values that make it a lot more
stable. The hyperstd of the mean is set to 10,000, which makes the mean
entirely stable during the duration of the experiment. The values of $\alpha$
and $\beta$ are fixed regardless of $\sigma_0$ such that the standard deviation
is 0.001 (compare that to the size of the adaptations, around 0.0125), and the
hyperparameters of the standar deviation are stable during the experiment.

\section{Parametrization of the task and the model}
Some of the parameters of the model can be fixed by the physics and scales of
the simulated experiment.

Henceforth, distances are measured in meters and time in seconds. As a
consequence, forces should be measured in Newtons, but bare in mind that what
the model calls ``force'' is no such thing. Instead, ``forces'' in the model
are the distance (in meters) that the real force (in Newtons) would make the
arm move in some $\Delta t$, assuming a mass of 1Kg and zero initial
velocity. This simplification of the terms might come back to bite us in the
ass and might be replaced later. But first, let us speak of real forces.

The force exerted by the mechanical arm depends on the context. In the baseline
context, the force is 0N. Taking inspiration from \todo{Citation needed}, we
take the forces of the other two contexts to be \force and -\force,
respectively. We will assume that $\Delta t = \dt$ \todo{Need to find a good
  way of fixing this}, which represents reaction, processing and decision
times.

Back to the lazy encoding of forces in the model, one can calculate the effects
of these forces on the arm. With a force of 10N and a $\Delta t$ of 50ms, the
total distance the arm will be moved by the force (again, assuming mass of 1Kg)
is given by:
\begin{align}
  d_\pm &= \pm \frac{1}{2}(\force / 1Kg)((\dt)^2)\\
               &= \pm 0.0125m
\end{align}
The agent will try to estimate $d_\pm$ during the experiment.

We will assume \todo{This is arbitrary. Might want to justify it later.} that
the participant observes the position of her own hand (visual + proprioceptive)
with Gaussian observation noise with $\mu = 0m$ and $\sigma = 0.005m$.

\section{Simulation results}
\subsection{Deadaptation and context inference}
During the deadaptation phase, not all parametrized agents detect the correct
context. For each agent type (LR, LRS, LRMS), different parameter values allow
the information from the predicted hand positions to overcome the information
from the cue, which enables the correct identification of the current context.

It is unclear which parameter values lead to this, but, as expected, prediction
noise (see XXX) and context noise (see XXX) play an important role. In
particular, keeping the prediction noise below 0.2 seems to be necessary for
the correct identification of the context, for all the used values of context
noise. It could be good to check this.

\subsubsection{Example: LR model with bad cues}
Using the LR models with only two contexts (baseline, red) creates slow
adaptation (across 400 trials or so), with fast-but-not-immediate switch back
to baseline when provided with false cues: after the adaptation block ends, a
baseline block ends but the cue still points to the adaptation block.

Simulations from this can be seen in the function
slow\_context\_inference\_badcues of /adaptation/code/simulations.py

The agent parameters are as follows:

\begin{center}
\begin{tabularx}{0.8\textwidth}{ c | c | X}
  Variable & Value & Description \\ \hline
  prediction\_noise & 0 & Setting this to a nonzero value screws everything up. May be a bug. \\  
  cue\_noise & 0.015 & Changing this by 0.001 creates slower or faster context inference \\
  force\_sds & 0.0002 & Changing this makes the model learn more quickly or slowly. With this value, the agent barely finishes adaptation by the time the adaptation block is over. \\
  context\_noise & 0 & Higher numbers make context inference more uncertain after evidence accumulation.
\end{tabularx}
\end{center}

\subsubsection{Example: LR model with no cues}
Using the LR models with only two contexts (baseline, red) creates slow
adaptation (across 400 trials or so), with fast-but-not-immediate switch back
to baseline when no cues are provided. After each block (A or O), a slow switch
can be observed, which is based entirely on the internal models producing noisy
predictions.

Simulations from this can be seen in the function
slow\_context\_inference\_badcues of /adaptation/code/simulations.py

\begin{center}
\begin{tabularx}{0.8\textwidth}{ c | c | X}
  Variable & Value & Description \\ \hline
  prediction\_noise & 3 & The bigger this is, the slower correct inference is. Only with no cues, though. \\  
  cue\_noise & 0.5 & Creates uninformative cues. \\
  force\_sds & 0.0002 & Changing this makes the model learn more quickly or slowly. With this value, the agent barely finishes adaptation by the time the adaptation block is over. \\
  context\_noise & 0 & Higher numbers make context inference more uncertain after evidence accumulation.  
\end{tabularx}
\end{center}



\subsection{Context inference requires a new likelihood}
\label{subsec:new-likelihood}
It became clear by running preliminary simulations that, regardless of the
agent, context inference happened too quickly and too ``reliably''. The context
was identified during the first trial at first, and during deadaptation, when
the cue and the forces do not match, it took at most a couple of trials to
recognize the real context.

Since the model proposes that deadaptation is nothing but a consequence of slow
recognition of the context, lightning-fast context recognition will just not do
it.

The problem stems from the fact that the estimate on the magnitudes becomes so
good (e.g. low standard deviation on the estimate) that the likelihood
$p(x | C_t)$ becomes a delta function. Because of this, the cue itself is being
ignored and the context is immediately identified.

The best fix to this problem is to modify the way the likelihood works. As it
stands, the likelihood is the probability of observing the current hand position
given the current context and the previous hand position. The more ``separated''
the different estimates become from each other (the means are too far apart
and/or the precisions are too high), the more this is going to be a delta
function.

The best fix for this is to modify the likelihood function and introduce in it a
measurement of the reliability of each of the modalities of perception (force
feedback and visual cue). Such moderators are common in the literature
\citep[e.g.][]{Lee_Neural_2014}.

The simplest implementation I can think of is
one in which additive noise is added to the posterior estimate for each
modality:
\begin{equation}
p(C | \omega_t) \leftarrow (p(C | \omega_t) + \chi_{\omega})
p(C | s_{t-1}, s_t, a_{t-1}) \leftarrow (p(C |  s_{t-1}, s_t, a_{t-1}) + \chi_{\text{pred}})
\end{equation}
where $\chi_{\omega}$ and $\chi_{\text{pred}}$ are constants which encode the
reliability of each modality.

A more convoluted approach would be to bring all modalities to the same
level. In the current implementation, the likelihood uses the standard deviation
of the inference over the magnitudes to calculate $p(C | s, a)$. Instead, a
standard deviation could be used that is proportional to the mean, which would
put the cue and predictive modalities on the same playing ground, depending on
the proportionality constant chosen.

\subsection{\cite{Davidson_Scaling_2004} results}
Running simulations I realized that the model, as is, will not be able to
reproduce the results from \cite{Davidson_Scaling_2004}. To do so, the model
would need to be modified to create new models online, but only one at a
time. A model would be created and kept at an adaptation of zero with very high
standard deviation, and this model would very naturally learn any adaptation
that is not explained by any other model.

The biggest problem is in reproducing experiment 2, group 1 of
\cite{Davidson_Scaling_2004}, in which the adaptations that will be presented
are A and 3A. In this case, at the beginning there should be an empty model
that learns A. Once this model is far away from baseline, a new empty model
should be created (and kept ``on call''), which eventually would pick up on
3A. If the two empty models are created at the same time, both will learn A,
and later only one of them (picked randomly) will learn 3A, while the other
stays at A; this is fine for simulations but difficult to explain from a
mechanistic point of view: why would the brain train two models to do the same
thing?

Possible solutions to this would be: (1) as said before, that only one empty
model exists at any time. (2) any model would have an empty counterpart that
can always learn faster. (3) Context inference would be able to pick up on low
model evidence (e.g. when no internal model explains the observations at all)
and trigger the creation of a new model, either always as a copy of baseline or
as a copy of the closest to the observations.

This is work for future studies.

\section{Conclusions}



\end{chapter}






\bibliography{../MyLibrary}

\end{document}

